{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8406d011-be4c-42c9-bfa2-501cc34ac4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thi is a sampl tweet check it out refugeesnotwelcom refuge welcom\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    # Remove characters: | : , ; & ! ? \\\n",
    "    tweet = re.sub(r'[|:;,;&!?\\\\]', ' ', tweet)\n",
    "\n",
    "    # Apply lowercase\n",
    "    tweet = tweet.lower()\n",
    "\n",
    "    # Apply stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tweet = ' '.join([stemmer.stem(word) for word in tweet.split()])\n",
    "\n",
    "    # Remove tokens with document frequency less than 5\n",
    "    # Assuming you have a function remove_low_df_tokens(tweet, min_df) to implement this step\n",
    "\n",
    "    # Normalize hashtags into words\n",
    "    tweet = re.sub(r'#(\\w+)', r'\\1', tweet)\n",
    "\n",
    "    return tweet\n",
    "\n",
    "# Example usage\n",
    "tweet = \"This is a #sample tweet! | Check it out: refugeesnotwelcome, refugees welcome!\"\n",
    "processed_tweet = preprocess_tweet(tweet)\n",
    "print(processed_tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26fcb641-ffb3-47e7-aa44-210490a49ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "# Load the pre-trained Google News word embeddings\n",
    "word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5205e1e-80d4-471a-b76c-671879eb43b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 03:23:10.676626: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-15 03:23:10.972273: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-15 03:23:10.974044: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-15 03:23:12.217535: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-12-15 03:23:14.065427: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-15 03:23:14.091491: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-12-15 03:23:14.140085: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 3600001200 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, GRU, Dense\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# Define the maximum sequence length (100 words as mentioned in the paper)\n",
    "max_sequence_length = 100\n",
    "# Define the dropout layer\n",
    "dropout_layer = Dropout(rate=0.2)\n",
    "\n",
    "# Define the 1D convolutional layer\n",
    "cnn_layer = Conv1D(filters=100, kernel_size=4, activation='relu', padding='same')\n",
    "\n",
    "# Define the embedding layer using the pre-trained word embeddings\n",
    "embedding_layer = Embedding(input_dim=len(word_vectors.index_to_key) + 1,\n",
    "                            output_dim=300,\n",
    "                            weights=[word_vectors.vectors],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False,\n",
    "                            mask_zero=True)\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(dropout_layer)\n",
    "model.add(cnn_layer)\n",
    "model.add(gru_layer)\n",
    "model.add(Dense(units=1, activation='sigmoid'))  # Output layer for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c48f29-1a9f-418b-bdf2-0c9a91e31a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the updated model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9408a243-13f8-4b62-a571-5846d0545863",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
